{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed666e1-5475-414e-97d4-cec1843d0735",
   "metadata": {},
   "source": [
    "In generated cleaned PDB files, there are gaps within chains.\n",
    "\n",
    "Create FASTA and SASA files that contain gap character `-` where needed, \\\n",
    "so working with such data is as convenient as possible later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967e4d8e-79cb-483c-a6ac-99d61f481f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as c\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.PDBExceptions import PDBConstructionWarning\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import _file_paths as fp\n",
    "\n",
    "MAXIMUM_CHAIN_LEN = 300\n",
    "PROGRESS_CHECK_004_PATH = 'progress_check_004_data.p'\n",
    "\n",
    "try:\n",
    "    progress = pickle.load(open(PROGRESS_CHECK_004_PATH, 'rb'))\n",
    "except FileNotFoundError:\n",
    "    progress = {\n",
    "        'processed': set(),\n",
    "        'cons_sasa_alignment': dict(L=[0] * MAXIMUM_CHAIN_LEN, H=[0] * MAXIMUM_CHAIN_LEN),\n",
    "        'cons_sasa_counts': dict(L=c.defaultdict(int), H=c.defaultdict(int))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8aa1a6d9-db81-4fa9-b2a8-c70a304c7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress['cons_sasa_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98895581-7ac0-4888-982c-b069e7023c37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364da11acac041028cbc36e85f6b19d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating aligned data files:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/sequences/structure-chains/nxy..fasta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9g/cdlp4k4d01n7v324hv9rc_gr0000gn/T/ipykernel_4228/1902719395.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0msasa_alignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sasa_alignment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchain_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malignment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msasa_alignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9g/cdlp4k4d01n7v324hv9rc_gr0000gn/T/ipykernel_4228/1902719395.py\u001b[0m in \u001b[0;36mhandle_file\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 structure_code + '.fasta')\n\u001b[1;32m     75\u001b[0m             fasta_sequences = SeqIO.parse(\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasta_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 'fasta')\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# write fasta data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/sequences/structure-chains/nxy..fasta'"
     ]
    }
   ],
   "source": [
    "SASA_EMPTY_VALUE = np.nan\n",
    "\n",
    "\n",
    "def write_structure_property_files(dir_path: str, structure_code: str, l: list, h: list):\n",
    "    l = [str(x) for x in l]\n",
    "    h = [str(x) for x in h]\n",
    "    with open(os.path.join(dir_path, 'light', structure_code + '.csv'),\n",
    "              'w', encoding='utf-8') as light_file:\n",
    "        light_file.write(' '.join(l) + os.linesep)\n",
    "        \n",
    "    with open(os.path.join(dir_path, 'heavy', structure_code + '.csv'),\n",
    "              'w', encoding='utf-8') as heavy_file:\n",
    "        heavy_file.write(' '.join(h) + os.linesep)\n",
    "        \n",
    "    with open(os.path.join(dir_path, 'merged', structure_code + '.csv'),\n",
    "              'w', encoding='utf-8') as merged_file:\n",
    "        merged_file.write(' '.join(l) + os.linesep)   \n",
    "        merged_file.write(' '.join(h) + os.linesep)                    \n",
    "\n",
    "        \n",
    "def generate_sasa_heatmap(data: dict, output_file_path: str):\n",
    "    plt.clf()\n",
    "    sns.set(rc={'figure.figsize':(20, 2)})\n",
    "    ax = sns.heatmap(data)\n",
    "    plt.savefig(output_file_path)\n",
    "\n",
    "    \n",
    "def handle_file(file_name: str) -> dict:\n",
    "    input_file_path = os.path.join(fp.PDB_CLEANED_DIR_PATH, file_name)\n",
    "    \n",
    "    structure_code = input_file_path[-8:-4]\n",
    "    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore', PDBConstructionWarning) \n",
    "            \n",
    "            # load PDB file\n",
    "            pp = PDBParser()\n",
    "            structure = pp.get_structure(structure_code, input_file_path)\n",
    "\n",
    "            # prepare numbering data\n",
    "            chain_positions = dict(L=list(), H=list())\n",
    "            for chain in structure.get_chains():\n",
    "                prev = 0\n",
    "                for residue in chain.get_residues():\n",
    "                    pos = residue.get_id()[1]\n",
    "                    for p in range(prev+1, pos):\n",
    "                        chain_positions[chain.id].append('-')\n",
    "                    chain_positions[chain.id].append(pos)\n",
    "                    prev = pos\n",
    "            # write numbering data\n",
    "            structure_code = input_file_path[-8:-4]\n",
    "            write_structure_property_files(\n",
    "                fp.POSITIONS_ALIGNED_DIR_PATH, \n",
    "                structure_code,\n",
    "                chain_positions['L'],\n",
    "                chain_positions['H'])\n",
    "\n",
    "            # prepare fasta data\n",
    "            fasta_file = os.path.join(\n",
    "                fp.AA_RAW_DIR_PATH,\n",
    "                structure_code + '.fasta')\n",
    "            fasta_sequences = SeqIO.parse(\n",
    "                open(fasta_file, 'r', encoding='utf-8'), \n",
    "                'fasta')\n",
    "            # write fasta data\n",
    "            fasta_dashes = dict(L='', H='')\n",
    "            for seq in fasta_sequences:\n",
    "                fasta_dashes[seq.id[-1]] = str(seq.seq).replace('X', '-')\n",
    "            write_structure_property_files(\n",
    "                fp.AA_ALIGNED_DIR_PATH, \n",
    "                structure_code,\n",
    "                fasta_dashes['L'],\n",
    "                fasta_dashes['H'])\n",
    "\n",
    "            # prepare sasa data\n",
    "            sasa_alignment = dict(L=[], H=[])\n",
    "            sasa_file_path = os.path.join(\n",
    "                fp.SASA_RAW_DIR_PATH, \n",
    "                structure_code + '.txt')\n",
    "            with open(sasa_file_path, 'r', encoding='utf-8') as sasa_file:\n",
    "                for chain_id, sasa_line in zip(['L', 'H'], sasa_file):\n",
    "                    sasa_line_i = iter(sasa_line.split())\n",
    "                    for pos in chain_positions[chain_id]:\n",
    "                        if pos == '-':\n",
    "                            sasa_alignment[chain_id].append(SASA_EMPTY_VALUE)\n",
    "                        else:\n",
    "                            sasa_alignment[chain_id].append(next(sasa_line_i))\n",
    "            for chain_id, alignment in sasa_alignment.items():\n",
    "                sasa_alignment[chain_id] = [float(x) for x in alignment] \n",
    "                \n",
    "            # write sasa data\n",
    "            write_structure_property_files(\n",
    "                fp.SASA_ALIGNED_DIR_PATH,\n",
    "                structure_code,\n",
    "                sasa_alignment['L'],\n",
    "                sasa_alignment['H'])\n",
    "            \n",
    "            # create SASA visualization\n",
    "            # 1) append zeroes to the shorter chain\n",
    "            max_len = max(len(alignment) for alignment in sasa_alignment.values())\n",
    "            for chain_id, alignment in sasa_alignment.items():\n",
    "                if max_len > len(alignment):\n",
    "                    sasa_alignment[chain_id] += ([SASA_EMPTY_VALUE] * (max_len-len(alignment)))\n",
    "            sasa_data = np.array([sasa_alignment['L'], sasa_alignment['H']])\n",
    "            # 2) generate the heatmap\n",
    "            heatmap_path = os.path.join(fp.SASA_VIZ_DIR_PATH, structure_code+'.png')\n",
    "            generate_sasa_heatmap(sasa_data, heatmap_path)\n",
    "            \n",
    "            return {'sasa_alignment': sasa_alignment}\n",
    "\n",
    "import collections as c           \n",
    "consensual_sasa_alignment = progress['cons_sasa_alignment']\n",
    "cons_sasa_counts = progress['cons_sasa_counts']\n",
    "file_names = os.listdir(fp.PDB_CLEANED_DIR_PATH)\n",
    "\n",
    "for file_name in tqdm(file_names, desc='Generating aligned data files'):\n",
    "    if file_name.startswith('.'): continue\n",
    "    if file_name in progress['processed']: continue\n",
    "    if file_name.endswith('.pdb'): continue\n",
    "\n",
    "    try:\n",
    "        stats = handle_file(file_name)\n",
    "        sasa_alignment = stats['sasa_alignment']\n",
    "        for chain_id, alignment in sasa_alignment.items():\n",
    "            for index, sasa in enumerate(alignment):\n",
    "                if sasa == SASA_EMPTY_VALUE: continue\n",
    "                consensual_sasa_alignment[chain_id][index] += sasa\n",
    "                cons_sasa_counts[chain_id][index] += 1\n",
    "        \n",
    "        # save progress\n",
    "        progress['cons_sasa_alignment'] = consensual_sasa_alignment\n",
    "        progress['cons_sasa_counts'] = cons_sasa_counts\n",
    "        progress['processed'].add(file_name)\n",
    "        pickle.dump(progress, open(PROGRESS_CHECK_004_PATH, 'wb'))\n",
    "    except ValueError as e:\n",
    "        print(file_name, e, type(e))\n",
    "    except StopIteration as e:\n",
    "        print(type(e))\n",
    "        \n",
    "for chain_id, alignment in consensual_sasa_alignment.items():\n",
    "    for index, sasa in enumerate(alignment):\n",
    "        if cons_sasa_counts[chain_id][index] == 0: continue\n",
    "        consensual_sasa_alignment[chain_id][index] /= cons_sasa_counts[chain_id][index]\n",
    "                                 \n",
    "csa = consensual_sasa_alignment\n",
    "summary_heatmap_path = os.path.join(fp.SASA_VIZ_DIR_PATH, 'all.png')\n",
    "generate_sasa_heatmap(np.array([csa['L'], csa['H']]), summary_heatmap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e553508-13b8-4fe6-89b8-ba59b6750f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26e16173e3145228060e889335c9a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning raw PDB data...:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# renumbering from RAW to AHO pdb\n",
    "\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import _file_paths as fp\n",
    "import _renumber\n",
    "\n",
    "PROGRESS_CHECK_PATH = 'progress_check_004_renumber_aho.p'\n",
    "progress = _renumber.get_progress(progress_check_path=PROGRESS_CHECK_PATH)    \n",
    "file_names = os.listdir(fp.PDB_CLEANED_SCHEME_RAW_DIR_PATH)\n",
    "\n",
    "for file_name in tqdm(file_names, desc='Cleaning raw PDB data...'):\n",
    "    _renumber.renumber(input_dir_path=fp.PDB_CLEANED_SCHEME_RAW_DIR_PATH, \n",
    "             file_name=file_name,\n",
    "             output_dir_path=fp.PDB_CLEANED_SCHEME_AHO_DIR_PATH,\n",
    "             numbering_scheme='aho',\n",
    "             progress=progress, \n",
    "             progress_check_path=PROGRESS_CHECK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a64e00e4-8d75-49a7-b25d-a1e42f902837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1766aae7441d4a44b875c739f5fd4586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning raw PDB data...:   0%|          | 0/4251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unecessary\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import _file_paths as fp\n",
    "\n",
    "file_names = os.listdir(fp.AA_RAW_DIR_PATH)\n",
    "for file_name in tqdm(file_names, desc='Cleaning raw PDB data...'):\n",
    "    if not file_name.endswith('.fasta'):\n",
    "        continue\n",
    "    input_path = os.path.join(fp.AA_RAW_DIR_PATH, file_name)\n",
    "    output_path = os.path.join('../../data/csv/anarci-from-fasta/', file_name[:-6]) # without extension\n",
    "    cmd = ['anarci', '-i', input_path, \n",
    "           '--scheme', 'aho', \n",
    "           '-o', output_path, \n",
    "           '--csv']\n",
    "    code = subprocess.call(cmd)\n",
    "    if code != 0:\n",
    "        print(file_name, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ccc38-9384-4e87-a2da-688073b1326e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
